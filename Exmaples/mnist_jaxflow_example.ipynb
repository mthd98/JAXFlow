{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2303ed92",
   "metadata": {},
   "source": [
    "# MNIST Classification with JaxFlow\n",
    "This notebook demonstrates how to build and train a Convolutional Neural Network on the MNIST dataset using JaxFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16aa121",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade jaxflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9974ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jaxflow as jf\n",
    "from jaxflow.models import Model\n",
    "from jaxflow.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from jaxflow.initializers import GlorotUniform, Zeros\n",
    "from jaxflow.optimizers import Adam\n",
    "from jaxflow.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88023aad",
   "metadata": {},
   "source": [
    "## 1. Load and preprocess MNIST\n",
    "We first load the MNIST dataset and normalize pixel values to the [0, 1] range. We also add a channel dimension for compatibility with Conv2D layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ac0b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess MNIST\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype(jnp.float32) / 255.0\n",
    "x_test = x_test.astype(jnp.float32) / 255.0\n",
    "x_train = x_train[..., None]\n",
    "x_test = x_test[..., None]\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412a4eb6",
   "metadata": {},
   "source": [
    "## 2. Define the CNN model\n",
    "We define a simple CNN with two convolutional blocks followed by a fully connected layer and an output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0b7b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(Model):\n",
    "    def __init__(self, num_classes: int = 10, name: str = \"MyCNN\"):\n",
    "        super().__init__(name=name)\n",
    "        self.conv1 = Conv2D(filters=32, kernel_size=(3,3), activation=jf.activations.relu, kernel_initializer=GlorotUniform, bias_initializer=Zeros)\n",
    "        self.pool1 = MaxPooling2D(pool_size=(2,2))\n",
    "        self.conv2 = Conv2D(filters=64, kernel_size=(3,3), activation=jf.activations.relu, kernel_initializer=GlorotUniform, bias_initializer=Zeros)\n",
    "        self.pool2 = MaxPooling2D(pool_size=(2,2))\n",
    "        self.flatten = Flatten()\n",
    "        self.dense1 = Dense(units=64, activation=jf.activations.relu, kernel_initializer=GlorotUniform, bias_initializer=Zeros)\n",
    "        self.outputs = Dense(units=num_classes, activation=jf.activations.softmax, kernel_initializer=GlorotUniform, bias_initializer=Zeros)\n",
    "    def call(self, inputs, training: bool = False):\n",
    "        x = self.conv1(inputs, training=training)\n",
    "        x = self.pool1(x, training=training)\n",
    "        x = self.conv2(x, training=training)\n",
    "        x = self.pool2(x, training=training)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x, training=training)\n",
    "        return self.outputs(x, training=training)\n",
    "\n",
    "# Build the model\n",
    "model = CNN(num_classes=10)\n",
    "model.build(input_shape=(None, 28, 28, 1))\n",
    "print(model.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a098bc4",
   "metadata": {},
   "source": [
    "## 3. Compile and train the model\n",
    "We use the Adam optimizer and sparse categorical crossentropy loss. We train for 5 epochs with a batch size of 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8828ab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=0.001)\n",
    "loss_fn = SparseCategoricalCrossentropy()\n",
    "model.compile(optimizer=optimizer, loss=loss_fn,)\n",
    "history = model.fit(x_train, y_train, epochs=5, batch_size=128, validation_data=(x_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcc77fd",
   "metadata": {},
   "source": [
    "## 4. Evaluate on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a405530a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print(f\"Test Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e51ffc0",
   "metadata": {},
   "source": [
    "## 5. Plot training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a5ea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(history['loss'])+1)\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, history['loss'], label='Train Loss')\n",
    "plt.plot(epochs, history['val_loss'], label='Val Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3fd3a0",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "This notebook showed how to train a CNN on MNIST with JaxFlow. Feel free to experiment with different architectures, learning rates, and batch sizes to improve performance!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
